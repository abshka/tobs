# TOBS - Example environment variables
# Copy this file to `.env` and fill in real values before running the app.
# Do NOT commit your real .env to version control.

# ------------------------------------------------------------------------
# Required credentials
# ------------------------------------------------------------------------
API_ID=your_api_id_here
API_HASH=your_api_hash_here
PHONE_NUMBER=+1234567890

# Session name for Telethon (session file will be created under sessions/)
SESSION_NAME=tobs_session

# Export paths
EXPORT_PATH=./export
MEDIA_SUBDIR=media
CACHE_SUBDIR=cache
MONITORING_SUBDIR=monitoring

# ------------------------------------------------------------------------
# Performance & behavior
# ------------------------------------------------------------------------
# Options: conservative | balanced | aggressive | custom
PERFORMANCE_PROFILE=balanced

# ðŸ§µ Unified Thread Pool (TIER B - B-1)
# Maximum threads for all CPU-bound and I/O operations
# 0 = auto-detect (CPU cores * 1.5 for mixed workload)
# Recommended: leave at 0 for automatic tuning
MAX_THREADS=0

# Message fetching (Telegram API)
BATCH_FETCH_SIZE=100         # Max messages per API request (Telegram limit ~100)
PREFETCH_LOOKAHEAD=2         # Double-buffering lookahead for prefetch (if used)

# Async media download (background media downloader)
ASYNC_MEDIA_DOWNLOAD=True
ASYNC_DOWNLOAD_WORKERS=0     # 0 = auto (derived from performance settings)

# ------------------------------------------------------------------------
# Async Pipeline (fetch -> process -> write)
# ------------------------------------------------------------------------
# Feature flag to enable async pipeline for +50% throughput improvement.
# Default: True (recommended for production after TIER A completion)
# When enabled:
#  - messages are fetched in batches, processed by a pool of workers, and written
#    by an ordered writer to preserve chronological order.
#  - tune worker counts and queue sizes for your machine / workload.
ASYNC_PIPELINE_ENABLED=True

# Worker sizing:
# - FETCH: number of concurrent fetcher coroutines (I/O bound)
# - PROCESS: number of parallel processing workers (0 = auto / derive from performance.workers)
# - WRITE: number of writer tasks (1 preserves per-entity ordering; >1 possible for multi-file parallelism)
ASYNC_PIPELINE_FETCH_WORKERS=1
ASYNC_PIPELINE_PROCESS_WORKERS=0
ASYNC_PIPELINE_WRITE_WORKERS=1

# Bounded queue sizes (for backpressure). Tune higher for high-throughput machines.
ASYNC_PIPELINE_FETCH_QUEUE_SIZE=64
ASYNC_PIPELINE_PROCESS_QUEUE_SIZE=256

# DC-aware routing settings (P1)
# Enable routing that prefers workers pre-warmed to the entity's datacenter
# for 10-20% latency reduction in multi-DC scenarios
# true | false (default: true, recommended)
DC_AWARE_ROUTING_ENABLED=true
# strategy: smart | sticky | round_robin
DC_ROUTING_STRATEGY=smart
# pre-warm worker sessions to entity DC before heavy fetching
DC_PREWARM_ENABLED=true
# timeout (seconds) to wait for pre-warm RPCs
DC_PREWARM_TIMEOUT=5

# ------------------------------------------------------------------------
# Parallel Media Processing (TIER B - B-3)
# ------------------------------------------------------------------------
# Enable parallel processing of media files (images, videos, audio)
# for +15-25% throughput improvement on media-heavy exports
# true | false (default: true, recommended)
PARALLEL_MEDIA_PROCESSING=true

# Max concurrent media operations (0 = auto: CPU cores / 2)
# Conservative default ensures stability. Increase for better performance:
#  - 2: safe for 2-4 core machines
#  - 4: recommended for 4-8 core machines
#  - 8: high-performance for 8+ core machines
MAX_PARALLEL_MEDIA=0

# Memory limit for parallel processing (MB, default: 2048)
# Processing will throttle if memory exceeds this limit
# Increase if you have abundant RAM (8GB+ recommended for 4+ concurrent operations)
PARALLEL_MEDIA_MEMORY_LIMIT_MB=2048

# ------------------------------------------------------------------------
# BloomFilter Optimization (TIER B - B-4)
# ------------------------------------------------------------------------
# Dynamic sizing of BloomFilter for resume/deduplication functionality
# Automatically adjusts memory usage based on chat size for optimal performance

# Size multiplier for expected message count (default: 1.1 = 10% buffer)
# Adds buffer for new messages received during export
# Range: 1.0 - 1.5 (1.1 recommended for most use cases)
BLOOM_FILTER_SIZE_MULTIPLIER=1.1

# Minimum BloomFilter size (default: 10000 = ~120KB memory)
# Prevents over-allocation for small chats (<10k messages)
# Small chats use minimal memory while maintaining 1% false positive rate
BLOOM_FILTER_MIN_SIZE=10000

# Maximum BloomFilter size (default: 10000000 = ~12MB memory)
# Prevents excessive memory usage for mega-chats (>10M messages)
# Balances accuracy vs memory consumption for very large exports
BLOOM_FILTER_MAX_SIZE=10000000

# Memory impact examples:
# - 1k chat:   1.2MB â†’ 120KB  (90% reduction)
# - 10k chat:  1.2MB â†’ 120KB  (90% reduction)
# - 100k chat: 1.2MB â†’ 1.2MB  (optimal, no change)
# - 1M chat:   1.2MB â†’ 1.2MB  (optimal, no change)
# - 5M chat:   1.2MB â†’ 6MB    (acceptable for accuracy)
# - 20M chat:  1.2MB â†’ 12MB   (clamped to max, prevents OOM)

# ------------------------------------------------------------------------
# Zero-Copy Media Transfer (TIER B - B-2)
# ------------------------------------------------------------------------
# Enable zero-copy file transfer using os.sendfile() for +10-15% I/O improvement
# Uses kernel-level copying (2-3x faster for large files, -50-80% CPU usage)
# Platforms: Linux/macOS (sendfile), Windows (automatic fallback to aiofiles)
# true | false (default: true, recommended)
ZERO_COPY_ENABLED=true

# Minimum file size (MB) to use zero-copy (default: 10)
# Files smaller than this threshold use aiofiles (faster for small files)
# Recommended: 10MB for balanced performance
ZERO_COPY_MIN_SIZE_MB=10

# Verify file size after copy (default: true)
# Ensures data integrity by checking source and destination sizes match
# Disable for maximum speed (not recommended in production)
ZERO_COPY_VERIFY_COPY=true

# Chunk size (MB) for aiofiles fallback mode (default: 64)
# Used when zero-copy unavailable or file below min_size threshold
# Larger chunks = faster copy, but more memory usage
ZERO_COPY_CHUNK_SIZE_MB=64

# ------------------------------------------------------------------------
# InputPeer Cache (TIER C - C-3)
# ------------------------------------------------------------------------
# LRU cache with TTL for Telethon InputPeer objects
# Reduces redundant entity resolution API calls by caching InputPeer results
#
# INPUT_PEER_CACHE_SIZE: Maximum cached entities (default: 1000)
#   - For single chat: 100-500 is sufficient
#   - For batch exports: 1000-5000 recommended
#   - Memory impact: ~100 bytes per entry (~100KB for 1000 entries)
#
# INPUT_PEER_CACHE_TTL: Time-to-live in seconds (default: 3600 = 1 hour)
#   - Shorter TTL: More API calls, but fresher data
#   - Longer TTL: Fewer API calls, but stale data risk
#   - Recommended: 1800-7200 (30 min to 2 hours)
#
# Expected impact:
#   - 5-10% reduction in entity resolution API calls
#   - ROI: ~13.0 (highest among TIER C tasks)
#   - Typical hit rate: 60-80% after warmup
INPUT_PEER_CACHE_SIZE=1000
INPUT_PEER_CACHE_TTL=3600

# ------------------------------------------------------------------------
# TTY-Aware Modes (TIER B - B-5)
# ------------------------------------------------------------------------
# Control output mode based on terminal type:
# - 'auto': Auto-detect TTY mode (recommended, checks sys.stdout.isatty() + CI env vars)
# - 'force-tty': Always use rich output (colors, progress bars) - for manual testing
# - 'force-non-tty': Always use minimal JSON output - for CI/CD, pipes, redirects
#
# TTY mode (interactive terminal):
#   - Colored ANSI output with icons (âœ“/âœ—/âš /â„¹)
#   - Progress bars with â–ˆ characters
#   - Overwriting progress lines (\r)
#
# Non-TTY mode (pipe/redirect/CI):
#   - JSON lines for easy parsing
#   - No ANSI colors or control characters
#   - Machine-readable output format
#
# Example outputs:
# TTY:     â–¶ MyChat [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 55.0% (550/1000) - processing
# Non-TTY: {"type":"progress","entity_name":"MyChat","messages_processed":550,"total_messages":1000,"stage":"processing","percentage":55.0}
#
# Default: auto (recommended)
TTY_MODE=auto

# ------------------------------------------------------------------------
# Logging & runtime tuning
# ------------------------------------------------------------------------
LOG_LEVEL=INFO
LOG_BATCH_INTERVAL=5.0       # Batch interval for log rate-limiting (seconds, default 5.0)
EXPORT_BUFFER_SIZE=524288    # Buffered file writer size (bytes, default 512KB)
EXPORT_BUFFER_FLUSH_INTERVAL=1  # Seconds between periodic flushes (if configured)

# ------------------------------------------------------------------------
# Safety & notes
# ------------------------------------------------------------------------
# - Start with ASYNC_PIPELINE_ENABLED=False in production, then test on a single export
#   with realistic data and measure memory/throughput before enabling globally.
# - If you see unexpected behavior, flip ASYNC_PIPELINE_ENABLED back to False to
#   instantly fall back to the sequential exporter.
#
# Example quick start:
# 1) cp .env.example .env
# 2) Edit .env to add API_ID, API_HASH, PHONE_NUMBER
# 3) python main.py

# ------------------------------------------------------------------------
# VA-API Auto-Detection (TIER C-1)
# ------------------------------------------------------------------------
# Auto-detect VA-API (Video Acceleration API) hardware capabilities for
# hardware-accelerated video encoding. Uses 'vainfo' command to detect
# available encoders/decoders and driver information.
#
# Benefits:
#   - Video encoding: 5-10x faster (GPU vs CPU)
#   - CPU usage during video: -50-80% (offloaded to GPU)
#   - +3-5% throughput for video-heavy exports
#
# Override auto-detection and force CPU encoding (default: false)
# Set to 'true' if VA-API causes issues or for testing
FORCE_CPU_TRANSCODE=false

# Path to VA-API device (default: /dev/dri/renderD128)
# Most Intel/AMD GPUs use renderD128, but some systems may use renderD129
# Check with: ls -l /dev/dri/
VAAPI_DEVICE_PATH=/dev/dri/renderD128

